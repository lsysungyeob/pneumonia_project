{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1205167/2283401048.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet_model.load_state_dict(torch.load(unet_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (enc2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (enc3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (enc4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec4): Sequential(\n",
       "    (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec3): Sequential(\n",
       "    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec2): Sequential(\n",
       "    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec1): Sequential(\n",
       "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet import UNet\n",
    "import torch\n",
    "\n",
    "unet_model_path = \"unet_model.pth\"\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet_model = UNet().to(device)\n",
    "unet_model.load_state_dict(torch.load(unet_model_path))\n",
    "unet_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import RESNETDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "normal_dir = '../chest_xray/train/NORMAL'\n",
    "pneumonia_dir = '../chest_xray/train/PNEUMONIA'\n",
    "\n",
    "train_dataset = RESNETDataset(unet_model, device, normal_dir, pneumonia_dir, target_size=(256, 256))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "from resnet import ResNetCNN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 모델 저장 경로\n",
    "model_save_path = \"resnet_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 47.87099\n",
      "Model 1 saved to resnet_model.pth\n",
      "Epoch 2/30, Loss: 22.44897\n",
      "Model 2 saved to resnet_model.pth\n",
      "Epoch 3/30, Loss: 16.43862\n",
      "Model 3 saved to resnet_model.pth\n",
      "Epoch 4/30, Loss: 12.99708\n",
      "Model 4 saved to resnet_model.pth\n",
      "Epoch 5/30, Loss: 8.13874\n",
      "Model 5 saved to resnet_model.pth\n",
      "Epoch 6/30, Loss: 4.64698\n",
      "Model 6 saved to resnet_model.pth\n",
      "Epoch 7/30, Loss: 7.96268\n",
      "Epoch 8/30, Loss: 4.10742\n",
      "Model 8 saved to resnet_model.pth\n",
      "Epoch 9/30, Loss: 3.13135\n",
      "Model 9 saved to resnet_model.pth\n",
      "Epoch 10/30, Loss: 2.57380\n",
      "Model 10 saved to resnet_model.pth\n",
      "Epoch 11/30, Loss: 6.67355\n",
      "Epoch 12/30, Loss: 3.31020\n",
      "Epoch 13/30, Loss: 2.51941\n",
      "Model 13 saved to resnet_model.pth\n",
      "Epoch 14/30, Loss: 2.61249\n",
      "Epoch 15/30, Loss: 5.48285\n",
      "Epoch 16/30, Loss: 1.51200\n",
      "Model 16 saved to resnet_model.pth\n",
      "Epoch 17/30, Loss: 0.51173\n",
      "Model 17 saved to resnet_model.pth\n",
      "Epoch 18/30, Loss: 1.02467\n",
      "Epoch 19/30, Loss: 0.17930\n",
      "Model 19 saved to resnet_model.pth\n",
      "Epoch 20/30, Loss: 0.06144\n",
      "Model 20 saved to resnet_model.pth\n",
      "Epoch 21/30, Loss: 0.02911\n",
      "Model 21 saved to resnet_model.pth\n",
      "Epoch 22/30, Loss: 0.01987\n",
      "Model 22 saved to resnet_model.pth\n",
      "Epoch 23/30, Loss: 0.01685\n",
      "Model 23 saved to resnet_model.pth\n",
      "Epoch 24/30, Loss: 0.01058\n",
      "Model 24 saved to resnet_model.pth\n",
      "Epoch 25/30, Loss: 0.01455\n",
      "Epoch 26/30, Loss: 0.01014\n",
      "Model 26 saved to resnet_model.pth\n",
      "Epoch 27/30, Loss: 0.01530\n",
      "Epoch 28/30, Loss: 0.00673\n",
      "Model 28 saved to resnet_model.pth\n",
      "Epoch 29/30, Loss: 0.00600\n",
      "Model 29 saved to resnet_model.pth\n",
      "Epoch 30/30, Loss: 0.00385\n",
      "Model 30 saved to resnet_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "min = 999\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.repeat(1, 3, 1, 1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.5f}\")\n",
    "    if min > epoch_loss:\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model {epoch + 1} saved to {model_save_path}\")\n",
    "        min = epoch_loss\n",
    "\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#torch.save(model.state_dict(), model_save_path)\n",
    "#print(f\"Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
